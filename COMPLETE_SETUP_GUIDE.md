# Smart Assessment System - Complete Setup Guide

**Last Updated:** February 21, 2026  
**Version:** 1.0.0

---

## ğŸ“‹ Table of Contents

1. [System Overview](#system-overview)
2. [Architecture](#architecture)
3. [Technology Stack](#technology-stack)
4. [Prerequisites](#prerequisites)
5. [Local Setup Guide](#local-setup-guide)
6. [Running the Application](#running-the-application)
7. [Testing the System](#testing-the-system)
8. [API Documentation](#api-documentation)
9. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ System Overview

**Smart Assessment** is an AI-powered academic assessment system that:

- **Ingests** academic documents (PDF, PPTX, DOCX) and extracts structured content
- **Processes** documents through AI-powered parsing, chunking, and embedding
- **Manages** academic hierarchy (Subjects â†’ Units â†’ Concepts)
- **Generates** intelligent exam papers with MCQs, short answers, and long questions
- **Retrieves** relevant content using hybrid semantic + keyword search
- **Aligns** content to curriculum concepts automatically

### Key Features

âœ… **Document Ingestion Pipeline**
- Multi-format support (PDF, PowerPoint, Word)
- Intelligent parsing with element classification
- Semantic chunking with section awareness
- Automatic embedding generation
- Table and image processing

âœ… **Academic Structure Management**
- Hierarchical organization (Subject â†’ Unit â†’ Concept)
- AI-powered syllabus normalization
- Content alignment to concepts
- Document management per subject

âœ… **Intelligent Search**
- Semantic vector search
- Hybrid search (FTS + vector)
- Context building for RAG
- Concept-based filtering

âœ… **Exam Generation**
- Pattern-based question generation
- MCQ with distractors
- Short and long answer questions
- Bloom's taxonomy integration
- Answer key generation

---

## ğŸ—ï¸ Architecture

The system follows a **3-tier architecture** with clear separation of concerns:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (React + Vite)                   â”‚
â”‚  - Teacher Portal (Login, Profile, Change Password)         â”‚
â”‚  - Subjects Management (List, Detail, Units, Concepts)      â”‚
â”‚  - Document Ingestion (Upload, Parse, Embed)                â”‚
â”‚  - Vectors Explorer (Elements/Chunks, Embeddings Status)    â”‚
â”‚  - Exam Generation (Blueprint â†’ Generate)                   â”‚
â”‚  - Exam Viewing (MCQ Papers, Subjective Papers)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ REST API (Port 5173)
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AI BACKEND (FastAPI)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LAYER 1: INGESTION PIPELINE                         â”‚   â”‚
â”‚  â”‚  - Document Parser (PDF/PPTX/DOCX)                   â”‚   â”‚
â”‚  â”‚  - Normalizer (Unicode, whitespace)                  â”‚   â”‚
â”‚  â”‚  - Image Captioner (GPT-4o vision)                   â”‚   â”‚
â”‚  â”‚  - Table Formatter (LLM-based)                       â”‚   â”‚
â”‚  â”‚  - Cleanup Engine (Headers, footers, artifacts)      â”‚   â”‚
â”‚  â”‚  - Element Classifier (TEXT/DIAGRAM/TABLE/CODE)      â”‚   â”‚
â”‚  â”‚  - Academic Classifier (Bloom's taxonomy)            â”‚   â”‚
â”‚  â”‚  - Section Path Builder                              â”‚   â”‚
â”‚  â”‚  - Smart Chunker (150-350 tokens, semantic split)    â”‚   â”‚
â”‚  â”‚  - Embedding Generator (all-MiniLM-L6-v2)            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LAYER 2: STRUCTURE & ALIGNMENT                      â”‚   â”‚
â”‚  â”‚  - Academic Hierarchy (CRUD: Subject/Unit/Concept)   â”‚   â”‚
â”‚  â”‚  - Structure AI (Gemini: syllabus normalization)     â”‚   â”‚
â”‚  â”‚  - Alignment Engine (Gemini: content â†’ concepts)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LAYER 3: SEARCH & CONTEXT                           â”‚   â”‚
â”‚  â”‚  - Semantic Search (Qdrant vector search)            â”‚   â”‚
â”‚  â”‚  - Hybrid Search (FTS + vector, RRF merge)           â”‚   â”‚
â”‚  â”‚  - Context Builder (RAG pipeline)                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LAYER 4: EXAM GENERATION                            â”‚   â”‚
â”‚  â”‚  - Blueprint Interpreter                             â”‚   â”‚
â”‚  â”‚  - Question Generator (Gemini: MCQ/Short/Long)       â”‚   â”‚
â”‚  â”‚  - Paper Assembler                                   â”‚   â”‚
â”‚  â”‚  - Answer Key Generator                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ (Port 8001)
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA LAYER                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  POSTGRESQL (Structure & Content Database)           â”‚   â”‚
â”‚  â”‚  - subjects, units, concepts                         â”‚   â”‚
â”‚  â”‚  - documents, parsed_elements, document_chunks       â”‚   â”‚
â”‚  â”‚  - exams, questions, question_sources                â”‚   â”‚
â”‚  â”‚  - aligned_elements, question_bank                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  QDRANT (Vector Database)                            â”‚   â”‚
â”‚  â”‚  - academic_chunks (main retrieval, 384-dim)         â”‚   â”‚
â”‚  â”‚  - academic_elements (element-level search)          â”‚   â”‚
â”‚  â”‚  - question_embeddings (deduplication)               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  FILE STORAGE (Local uploads/)                       â”‚   â”‚
â”‚  â”‚  - Original documents (PDF/PPTX/DOCX)                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Technology Stack

### Frontend
- **Framework:** React 19.2.0
- **Build Tool:** Vite 5.x
- **Styling:** TailwindCSS 4.x
- **State Management:** Redux Toolkit 2.x
- **Routing:** React Router DOM 7.x
- **HTTP Client:** Axios
- **UI Components:** Lucide React (icons)
- **Forms:** React Hook Form + Yup validation

### Backend (AI Backend)
- **Framework:** FastAPI 0.127.0
- **Language:** Python 3.x
- **ASGI Server:** Uvicorn 0.40.0
- **ORM:** SQLAlchemy 2.0+
- **Database Driver:** psycopg2-binary, asyncpg

### AI & ML
- **LLM:** Google Gemini (via OpenAI-compatible API)
- **Embeddings:** SentenceTransformers (all-MiniLM-L6-v2, 384-dim)
- **Document Parsing:** Unstructured library
- **PDF Processing:** pdfplumber, pdfminer.six
- **Office Docs:** python-pptx, python-docx
- **Vision:** GPT-4o (image captioning)
- **JSON Repair:** json-repair

### Databases
- **Relational:** PostgreSQL 15
- **Vector Store:** Qdrant (latest)
- **Full-Text Search:** PostgreSQL tsvector

### Infrastructure
- **Containerization:** Docker & Docker Compose
- **OS Support:** Windows, Linux, macOS

---

## âš™ï¸ Prerequisites

### Required Software

1. **Docker Desktop**
   - Version: Latest stable
   - Required for: PostgreSQL, Qdrant, Ollama containers
   - Download: https://www.docker.com/products/docker-desktop/

2. **Python**
   - Version: 3.9 or higher
   - Required for: AI backend
   - Download: https://www.python.org/downloads/

3. **Node.js & npm**
   - Version: Node.js 18+ (includes npm)
   - Required for: Frontend
   - Download: https://nodejs.org/

4. **Git** (optional, for cloning)
   - Version: Latest
   - Download: https://git-scm.com/downloads

### Required API Keys

1. **Google API Key (Gemini)**
   - Used for: Structure AI, alignment, exam generation
   - Get it from: https://makersuite.google.com/app/apikey
   - Set in: `ai-backend/.env`

### System Requirements

- **RAM:** 8 GB minimum, 16 GB recommended
- **Storage:** 10 GB free space
- **Internet:** Required for API calls and package installation

---

## ğŸš€ Local Setup Guide

### Step 1: Clone or Download the Project

```bash
# If using Git
git clone <repository-url>
cd smart-assessment

# Or download and extract ZIP, then navigate to folder
cd smart-assessment
```

### Step 2: Setup AI Backend

#### 2.1 Navigate to AI Backend

```bash
cd ai-backend
```

#### 2.2 Create Python Virtual Environment

**On Windows:**
```bash
python -m venv .venv
.venv\Scripts\activate
```

**On Linux/macOS:**
```bash
python3 -m venv .venv
source .venv/bin/activate
```

#### 2.3 Install Python Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

This installs:
- FastAPI + Uvicorn (web framework)
- SQLAlchemy + PostgreSQL drivers
- SentenceTransformers (embeddings)
- Unstructured + document parsers
- Qdrant client
- OpenAI client (for Gemini)
- And more...

#### 2.4 Create Environment File

Create `ai-backend/.env` from the example:

```bash
# Copy the example
cp .env.example .env

# Or on Windows
copy .env.example .env
```

Edit `.env` and configure (minimal required config):

```env
# Database (defaults are fine for local Docker)
POSTGRES_USER=academic_user
POSTGRES_PASSWORD=academic_pass
POSTGRES_DB=academic_structure
POSTGRES_HOST=localhost
POSTGRES_PORT=5432

# Qdrant (defaults are fine for local Docker)
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Google Gemini API Key (REQUIRED)
GOOGLE_API_KEY=your_google_api_key_here

# OpenAI API Key (optional, if using OpenAI instead of Gemini)
# OPENAI_API_KEY=your_openai_key_here
```

**Important:** Replace `your_google_api_key_here` with your actual Google API key.

### Step 3: Setup Frontend

#### 3.1 Navigate to Frontend

```bash
cd ../frontend  # From ai-backend directory
```

#### 3.2 Install Node Dependencies

```bash
npm install
```

This installs:
- React + React DOM
- Vite (build tool)
- TailwindCSS
- Redux Toolkit
- React Router
- Axios
- And more...

### Step 4: Start Docker Services

Docker Desktop must be running before this step.

#### 4.1 Navigate to AI Backend (if not there)

```bash
cd ../ai-backend  # From frontend directory
```

#### 4.2 Start Docker Compose

```bash
docker compose up -d
```

This starts three containers:
- **PostgreSQL** (port 5432) - Structure database
- **Qdrant** (port 6333) - Vector database
- **Ollama** (port 11434) - Local LLM runtime (optional)

#### 4.3 Verify Containers are Running

```bash
docker ps
```

You should see three containers:
- `academic_postgres`
- `academic_qdrant`
- `academic_ollama`

#### 4.4 Wait for Services to Initialize

Wait ~10-20 seconds for PostgreSQL and Qdrant to be fully ready.

### Step 5: Run Database Migrations

Database migrations create all required tables and indexes.

#### 5.1 From Project Root

**On Windows:**
```bash
cd ..  # Back to project root
# Edit run-migrations.sh and replace "source .venv/Scripts/activate" on line 10
# Or run manually:
cd ai-backend
.venv\Scripts\activate
python create_tables.py
python migrations/add_section_path_and_embedding_meta.py
python migrations/add_chunk_search_vector.py
```

**On Linux/macOS:**
```bash
cd ..  # Back to project root
./run-migrations.sh
```

This runs:
1. `create_tables.py` - Creates all database tables
2. Migration scripts - Adds columns, indexes, and constraints

Expected output:
```
=== Running DB migrations ===
1. Creating tables (create_tables.py)...
âœ“ Tables created successfully
2. Running migration: add_section_path_and_embedding_meta...
âœ“ Migration completed
3. Running migration: add_chunk_search_vector...
âœ“ FTS indexes created
=== Migrations complete ===
```

### Step 6: Create Qdrant Collections

Qdrant collections store vector embeddings for semantic search.

```bash
cd ai-backend
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/macOS

python create_qdrant_collection.py
```

This creates:
- `academic_chunks` - Main retrieval collection (384-dim vectors)
- `academic_elements` - Element-level search (optional)
- `question_embeddings` - Question deduplication

Expected output:
```
Creating Qdrant collections...
âœ“ Collection 'academic_chunks' created successfully
âœ“ Collection 'academic_elements' created successfully
âœ“ Collection 'question_embeddings' created successfully
```

---

## ğŸ® Running the Application

You need to run **two servers** simultaneously:

### Terminal 1: AI Backend (FastAPI)

```bash
cd ai-backend
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/macOS

python structure_api.py
```

Expected output:
```
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001
```

**Backend is now running at:** http://localhost:8001

Check health: http://localhost:8001/health

API docs: http://localhost:8001/docs

### Terminal 2: Frontend (React + Vite)

```bash
cd frontend
npm run dev
```

Expected output:
```
VITE v5.x.x  ready in 500 ms

âœ  Local:   http://localhost:5173/
âœ  Network: use --host to expose
```

**Frontend is now running at:** http://localhost:5173

### Access the Application

Open your browser and navigate to: **http://localhost:5173**

You should see the Smart Assessment login page.

---

## ğŸ§ª Testing the System

### Quick Health Checks

#### 1. Backend Health

```bash
curl http://localhost:8001/health
```

Expected response:
```json
{
  "status": "healthy",
  "service": "structure-api",
  "database": "connected"
}
```

#### 2. Search Health (Embeddings + Qdrant)

```bash
curl http://localhost:8001/search/health
```

Expected response:
```json
{
  "status": "healthy",
  "embedding_model": "all-MiniLM-L6-v2",
  "qdrant": "connected"
}
```

### End-to-End Flow Test

#### 1. Create Academic Structure

**Via UI:**
1. Go to **Ingest** page
2. Use **Structure AI** to normalize a syllabus
3. Create Subject â†’ Units â†’ Concepts

**Via API:**
```bash
# Create a subject
curl -X POST http://localhost:8001/subjects/ \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Operating Systems",
    "description": "Introduction to OS concepts",
    "code": "CS301"
  }'

# Create a unit (use subject_id from above response)
curl -X POST http://localhost:8001/units/ \
  -H "Content-Type: application/json" \
  -d '{
    "subject_id": 1,
    "name": "Process Management",
    "order_index": 1
  }'

# Create a concept (use unit_id from above response)
curl -X POST http://localhost:8001/concepts/ \
  -H "Content-Type: application/json" \
  -d '{
    "unit_id": 1,
    "name": "Process Scheduling",
    "order_index": 1
  }'
```

#### 2. Upload a Document

**Via UI:**
1. Go to **Subjects** â†’ Select your subject
2. Click **Upload Document**
3. Select a PDF/PPTX/DOCX file
4. Wait for processing (parsing â†’ chunking â†’ embedding)

**Via API (curl):**
```bash
curl -X POST http://localhost:8001/documents/upload-and-store \
  -F "file=@/path/to/your/document.pdf" \
  -F "subject_id=1"
```

This triggers the full ingestion pipeline:
- Parse â†’ Normalize â†’ Caption Images â†’ Format Tables
- Cleanup â†’ Classify â†’ Chunk â†’ Embed â†’ Store (DB + Qdrant)

#### 3. Semantic Search

```bash
curl -X POST http://localhost:8001/search/semantic \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is process scheduling?",
    "subject_id": 1,
    "limit": 5
  }'
```

Expected: Top 5 relevant chunks with similarity scores.

#### 4. Generate an Exam

**Via UI:**
1. Go to **Generate MCQ Exam** or **Generate Subjective Exam**
2. Select subject and units
3. Set question counts (MCQ, short, long)
4. Click **Generate**

**Via API:**
```bash
curl -X POST http://localhost:8001/exams/generate \
  -H "Content-Type: application/json" \
  -d '{
    "subject_id": 1,
    "unit_ids": [1],
    "counts": {
      "mcq": 5,
      "short": 2,
      "long": 1
    },
    "include_answer_key": true
  }'
```

Expected: Exam JSON with questions, options, and answer keys.

---

## ğŸ“š API Documentation

### Base URL

- **Local:** http://localhost:8001

### Interactive API Docs

- **Swagger UI:** http://localhost:8001/docs
- **ReDoc:** http://localhost:8001/redoc

### Core Endpoints Overview

#### Academic Structure

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/subjects/` | List all subjects |
| POST | `/subjects/` | Create a subject |
| GET | `/subjects/{id}` | Get subject details |
| GET | `/subjects/with-stats/all` | Subjects with doc counts |
| GET | `/subjects/{id}/with-documents` | Subject with units, concepts, documents |
| DELETE | `/subjects/{id}` | Delete subject (cascades) |
| POST | `/units/` | Create a unit |
| GET | `/units/{id}` | Get unit details |
| POST | `/concepts/` | Create a concept |
| GET | `/concepts/{id}` | Get concept details |

#### Structure AI

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/structure/normalize` | Normalize syllabus â†’ units + concepts |
| POST | `/structure/normalize-structured` | Structured output version |

#### Documents

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/documents/upload-and-store` | Upload & process document (full pipeline) |
| POST | `/documents/parse` | Parse only (no DB storage) |
| GET | `/documents/{id}` | Get document metadata |
| GET | `/documents/{id}/elements` | Get parsed elements |
| DELETE | `/documents/{id}` | Delete document + vectors |
| GET | `/documents/embedding-status` | Embedding status badge |
| GET | `/documents/elements-with-embeddings` | List elements (with filters) |
| GET | `/documents/chunks-with-embeddings` | List chunks (with filters) |

#### Alignment

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/alignment/align-document` | Align document elements â†’ concepts |
| POST | `/alignment/align-subjects` | Bulk align all documents in subjects |

#### Search

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/search/health` | Check search system health |
| POST | `/search/semantic` | Vector search (Qdrant) |
| POST | `/search/hybrid` | FTS + vector (RRF merge) |

#### Context

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/context/build` | Build RAG context from chunks |

#### Exams

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/exams/generate` | Generate exam from blueprint |
| GET | `/exams/` | List exams (by subject) |
| GET | `/exams/{id}` | Get exam with questions |

#### Question Bank (Layer 3)

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/questions/generate` | Generate question bank for concept/unit |
| GET | `/questions/` | List questions (with filters) |
| PATCH | `/questions/{id}` | Approve/reject/edit question |

---

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. Docker Containers Not Starting

**Problem:** `docker compose up -d` fails or containers exit immediately.

**Solutions:**
- Ensure Docker Desktop is running
- Check port conflicts (5432, 6333, 11434)
- View logs: `docker compose logs postgres` or `docker compose logs qdrant`
- Restart Docker Desktop
- Remove volumes and recreate: `docker compose down -v && docker compose up -d`

#### 2. Database Connection Error

**Problem:** Backend shows "could not connect to database"

**Solutions:**
- Verify PostgreSQL container is running: `docker ps`
- Check `.env` file has correct database credentials
- Wait 10-20 seconds after starting Docker for PostgreSQL to initialize
- Test connection: `psql -h localhost -U academic_user -d academic_structure`

#### 3. Qdrant Connection Error

**Problem:** Search endpoints fail with "Qdrant connection error"

**Solutions:**
- Verify Qdrant container is running: `docker ps`
- Check Qdrant UI: http://localhost:6333/dashboard
- Recreate collections: `python create_qdrant_collection.py`
- Check `.env` has correct `QDRANT_HOST` and `QDRANT_PORT`

#### 4. Embedding Model Download Slow

**Problem:** First run takes long to start (downloading model)

**Explanation:** SentenceTransformers downloads "all-MiniLM-L6-v2" (~90MB) on first use.

**Solutions:**
- Wait for download to complete (one-time, cached after)
- Check internet connection
- Model is cached in: `~/.cache/torch/sentence_transformers/`

#### 5. Frontend Cannot Connect to Backend

**Problem:** Frontend shows "Network Error" or CORS errors

**Solutions:**
- Ensure backend is running on port 8001: `curl http://localhost:8001/health`
- Check CORS settings in `structure_api.py` (should allow all origins in dev)
- Verify frontend is configured to use `http://localhost:8001` (check API base URL)

#### 6. Migration Script Fails

**Problem:** `run-migrations.sh` errors or tables not created

**Solutions:**
- Ensure database is running: `docker ps | grep postgres`
- Activate virtual environment first: `.venv\Scripts\activate`
- Run migrations manually one by one:
  ```bash
  python create_tables.py
  python migrations/add_section_path_and_embedding_meta.py
  python migrations/add_chunk_search_vector.py
  ```

#### 7. Google API Key Error

**Problem:** "GOOGLE_API_KEY environment variable not set"

**Solutions:**
- Create `.env` file in `ai-backend/`
- Add: `GOOGLE_API_KEY=your_actual_key_here`
- Get key from: https://makersuite.google.com/app/apikey
- Restart backend server after adding key

#### 8. Document Upload Fails

**Problem:** Upload returns 500 error or processing hangs

**Solutions:**
- Check file format is supported (PDF, PPTX, DOCX)
- Check file size is reasonable (<50MB recommended)
- View backend logs for specific error
- Ensure `uploads/` directory exists in `ai-backend/`
- Check disk space

#### 9. Windows-Specific Path Issues

**Problem:** Scripts fail on Windows with path errors

**Solutions:**
- Use Windows path format: `C:\Users\...` or forward slashes: `C:/Users/...`
- Replace `source .venv/bin/activate` with `.venv\Scripts\activate`
- Use `copy` instead of `cp` for file operations
- Run PowerShell or Command Prompt as Administrator if needed

#### 10. Port Already in Use

**Problem:** "Address already in use" error

**Solutions:**
- Frontend (5173): Kill process using port: `netstat -ano | findstr :5173` then `taskkill /PID <PID> /F`
- Backend (8001): Kill process using port: `netstat -ano | findstr :8001` then `taskkill /PID <PID> /F`
- Or change ports in:
  - Backend: `structure_api.py` (last line)
  - Frontend: `vite.config.js` (`server.port`)

### Getting Help

If issues persist:

1. **Check Logs:**
   - Backend: Terminal where `python structure_api.py` is running
   - Docker: `docker compose logs <service_name>`
   - Frontend: Browser console (F12 â†’ Console)

2. **Verify Environment:**
   - Python version: `python --version` (should be 3.9+)
   - Node version: `node --version` (should be 18+)
   - Docker: `docker --version`

3. **Reset Everything:**
   ```bash
   # Stop all services
   docker compose down -v
   
   # Remove Python cache
   rm -rf ai-backend/__pycache__
   rm -rf ai-backend/*/__pycache__
   
   # Restart from Step 4 (Docker setup)
   ```

---

## ğŸ“ Additional Notes

### Development Tips

1. **Hot Reload:**
   - Frontend: Vite auto-reloads on file changes
   - Backend: Restart `python structure_api.py` after code changes
   - Or use: `uvicorn structure_api:app --reload`

2. **Database Inspection:**
   - PostgreSQL: Use pgAdmin or connect via: `psql -h localhost -U academic_user -d academic_structure`
   - Qdrant: Web UI at http://localhost:6333/dashboard

3. **API Testing:**
   - Use Swagger UI: http://localhost:8001/docs
   - Or Postman/Insomnia with collections
   - Or curl commands from terminal

4. **Clearing Data:**
   ```bash
   # Clear database
   docker compose down -v
   docker compose up -d
   ./run-migrations.sh
   
   # Clear vectors
   python create_qdrant_collection.py  # Uses recreate=True
   ```

### Production Deployment

This guide is for **local development**. For production:

- Use proper secrets management (not `.env` files)
- Configure CORS properly (not `allow_origins=["*"]`)
- Use reverse proxy (nginx/Caddy)
- Enable HTTPS
- Use managed database services
- Add authentication/authorization
- Enable rate limiting
- Add monitoring and logging
- Use gunicorn/uvicorn workers for scaling

### License

Refer to project LICENSE file.

---

## ğŸ‰ Success!

If you've completed all steps, you should now have:

âœ… Docker containers running (PostgreSQL, Qdrant)  
âœ… AI Backend running on http://localhost:8001  
âœ… Frontend running on http://localhost:5173  
âœ… Database tables created  
âœ… Vector collections initialized  
âœ… System ready to ingest documents and generate exams  

**Next Steps:**
1. Upload your first document
2. Create subject structure
3. Generate an exam
4. Explore the API docs

Happy assessing! ğŸš€
