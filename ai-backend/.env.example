# ==========================================
# ACADEMIC INGESTION SYSTEM - LOCAL ENVIRONMENT
# All services run locally, no cloud APIs
# ==========================================

# ------------------------------------------
# POSTGRES (Structure Database - Source of Truth)
# ------------------------------------------
POSTGRES_USER=academic_user
POSTGRES_PASSWORD=academic_pass
POSTGRES_DB=academic_structure
POSTGRES_HOST=localhost
POSTGRES_PORT=5432

# ------------------------------------------
# QDRANT (Vector Database - Semantic Retrieval)
# ------------------------------------------
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_COLLECTION=concept_chunks

# ------------------------------------------
# OLLAMA (Local LLM - Classification Only)
# Model: Phi-3 Mini (must already be installed)
# ------------------------------------------
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_MODEL=phi3:mini

# Path to local Ollama models (reuse existing cache)
# Windows example: C:/Users/YourName/.ollama
# Linux/Mac example: ~/.ollama
OLLAMA_MODELS_PATH=~/.ollama

# ------------------------------------------
# EMBEDDING MODEL (Local - bge-small-en-v1.5)
# ------------------------------------------
EMBEDDING_MODEL=BAAI/bge-small-en-v1.5
EMBEDDING_DEVICE=cpu
# Set to 'cuda' if GPU available (NVIDIA GTX 1650)
# EMBEDDING_DEVICE=cuda

# ------------------------------------------
# API CONFIGURATION
# ------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=true

# ------------------------------------------
# SYSTEM CONSTRAINTS
# ------------------------------------------
# Maximum tokens for LLM context (HARD LIMIT)
MAX_LLM_TOKENS=500

# Confidence threshold for concept alignment
ALIGNMENT_CONFIDENCE_THRESHOLD=0.75
