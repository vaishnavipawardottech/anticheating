================================================================================
  SMART ASSESSMENT — AI BACKEND
  INGESTION & GENERATION PIPELINE DOCUMENTATION
================================================================================

Last updated: 2026-02-20
Author: AI Backend Team


--------------------------------------------------------------------------------
OVERVIEW
--------------------------------------------------------------------------------

The ai-backend has two major pipelines:

  1. INGESTION PIPELINE  — Processes uploaded academic documents (PDF, PPTX,
                           DOCX) into searchable, semantically-rich chunks that
                           are stored in PostgreSQL and indexed in Qdrant.

  2. GENERATION PIPELINE — Takes an exam pattern (free-text or PDF), interprets
                           it into a structured blueprint, retrieves relevant
                           content chunks for each question, and uses an LLM
                           (OpenAI GPT) to generate a complete exam paper.


================================================================================
PART 1 — INGESTION PIPELINE
================================================================================

Directory: ai-backend/ingestion/
Entry:     ingestion/__init__.py  (exports all step modules)

The ingestion pipeline is a 10-step sequential process. Each step is designed
to run independently with proper error handling.

────────────────────────────────────────────────────────────────────────────────
STEP 1: PARSE  (ingestion/parser.py → DocumentParser)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Accepts a file path (PDF, PPTX, or DOCX) and a filename.
    - Uses the `unstructured` library to extract raw semantic elements from
      the document in reading order.
    - Supported formats: .pdf, .pptx, .docx

  Key methods:
    - parse_document(file_path, filename)  → List[SemanticElement]
        Dispatches to _parse_pdf / _parse_pptx / _parse_docx based on extension.
    - _extract_element_data(element, order, source_filename)
        Converts each unstructured element into a SemanticElement schema object,
        preserving: element_type, text, page_number, order, metadata.
    - get_total_pages(elements)  → int | None
        Scans elements to find the max page number.

  Constraints:
    - DETERMINISTIC: Same file always produces the same output.
    - ISOLATED: No LLM calls, no external APIs, no embeddings.
    - Returns raw elements only — no chunking here.

  Output: List[SemanticElement]
    Each has: element_type, text, page_number, order, source_filename, metadata

────────────────────────────────────────────────────────────────────────────────
STEP 2: NORMALIZE  (ingestion/normalizer.py → normalize_text / normalize_elements)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Fixes unicode issues, removes invisible characters, normalizes whitespace.
    - Cleans up PDF/PPT artifacts (ligatures like "ﬁ" → "fi", etc.).
    - Operates in-place on each SemanticElement's text field.

  Output: List[SemanticElement]  (same list, text fields cleaned)

────────────────────────────────────────────────────────────────────────────────
STEP 3: CAPTION IMAGES  (ingestion/image_captioner.py → ImageCaptioner)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Detects image/figure elements in the parsed output.
    - Sends each image to GPT-4o to generate a descriptive text caption.
    - Replaces the empty or sparse image text with a rich description so the
      image content becomes searchable.

  LLM: OpenAI GPT-4o (vision capability)

  Output: List[SemanticElement]  (image elements now have descriptive text)

────────────────────────────────────────────────────────────────────────────────
STEP 4: FORMAT TABLES  (ingestion/table_formatter.py → TableFormatter)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Detects table elements from the parsed list.
    - Uses an LLM to convert garbled/raw table text into clean Markdown tables.
    - Properly formatted tables improve downstream chunking and retrieval.

  Output: List[SemanticElement]  (table elements now have clean Markdown text)

────────────────────────────────────────────────────────────────────────────────
STEP 5: CLEANUP  (ingestion/cleanup.py → DocumentCleanup / cleanup_elements)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Applies deterministic (no LLM) filters to remove noise:
        * Headers and footers (by element_type)
        * Standalone page numbers  (e.g., "5", "Page 12")
        * Table of Contents dotted leaders  (e.g., "Introduction ........ 5")
        * Pure numeric elements  (standalone numbers that are just noise)
        * Empty / whitespace-only elements
        * Bullet/symbol-only elements  (e.g., "□ □", "■", "•")
        * PDF CID encoding artifacts  (e.g., "(cid:0)")
    - Tracks and returns CleanupStatistics (counts by removal reason).

  Output: CleanupResult
    - .elements  → filtered List[SemanticElement]
    - .statistics → CleanupStatistics (counts by removal reason)

────────────────────────────────────────────────────────────────────────────────
STEP 6: CLASSIFY  (ingestion/classifier.py → ElementClassifier)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Assigns each element a coarse category: TEXT, TABLE, IMAGE, CODE, etc.
    - Pattern-based rules using element_type from unstructured.

  Output: List[SemanticElement]  (with category field populated)

────────────────────────────────────────────────────────────────────────────────
STEP 7: CHUNK  (ingestion/chunker.py)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Merges small/fragmented elements into retrieval-optimized chunks.
    - Chunks are 600–1000 characters with 100-character overlap.
    - Assigns section_path to each chunk (e.g., "Unit I > Introduction to HCI
      > Design Principles") by tracking heading hierarchy.
    - Tables are converted into row-level chunks for precise retrieval
      (one chunk per row, plus optional schema chunk for column meanings).

  Key functions:
    - prepare_for_chunking(elements)
        Drops junk tokens, merges sentence fragments, deduplicates consecutive
        identical text, normalizes whitespace.
    - compute_section_paths_for_elements(elements)
        Walks through elements, updating a section path stack when headings are
        encountered. Rejects headings that are >100 chars or look like body text.
        Strips institutional boilerplate from paths.
    - chunk_elements(elements)  → List[DocumentChunkInfo]
        Core chunking logic. Groups normalized elements into chunks respecting
        size bounds and section boundaries.
    - table_to_row_chunks(table_text, section_path, page, element_order)
        Converts a Markdown table into: one optional schema chunk + one chunk
        per data row.

  Target constants:
    TARGET_CHUNK_MIN_CHARS = 600
    TARGET_CHUNK_MAX_CHARS = 1000
    OVERLAP_CHARS         = 100

  Output: List[DocumentChunkInfo]
    Each has: text, section_path, page_start, page_end, source_element_orders,
              chunk_type, table_id, row_id

────────────────────────────────────────────────────────────────────────────────
STEP 8: EMBED  (embeddings/generator.py)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Generates vector embeddings for each chunk using OpenAI's
      text-embedding-3-small model (1536 dimensions).
    - Embeddings capture the semantic meaning of the chunk for similarity search.

  Output: List[List[float]]  (parallel to chunk list)

────────────────────────────────────────────────────────────────────────────────
STEP 9: INDEX  (embeddings/qdrant_manager.py → QdrantManager + PostgreSQL)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Stores chunk text and metadata in PostgreSQL (document_chunks table via
      SQLAlchemy models).
    - Indexes chunk embeddings in Qdrant vector database for semantic search.

  Qdrant collections:
    - academic_chunks   — Stores chunk embeddings + rich metadata payload.
                          This is the MAIN retrieval collection.
    - academic_elements — Stores element-level embeddings (special cases).
    - question_embeddings — Stores generated question embeddings for dedupe.

  Chunk payload fields in Qdrant:
    subject_id, unit_id, concept_id, section_path, blooms_level,
    blooms_level_int, difficulty, difficulty_score, section_type,
    source_type, usage_count, chunk_type, parent_id, document_id

  Key methods on QdrantManager:
    - create_collection(recreate=False)    — Creates collections with indexes.
    - index_chunks_batch(chunk_ids, embeddings, metadatas)  — Bulk upsert.
    - search(query_vector, filters...)     — Hybrid filter + vector search.
    - delete_by_document(document_id)      — Remove all vectors for a doc.

────────────────────────────────────────────────────────────────────────────────
STEP 10: ACADEMIC CLASSIFY  (ingestion/academic_classifier.py → classify_chunks)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Calls OpenAI GPT-3.5-turbo in batches of 10 chunks to classify each chunk
      with academic metadata:
        * section_type:    definition | example | derivation | exercise |
                           explanation | summary
        * source_type:     textbook | slides | notes | lab | reference
        * blooms_level:    remember | understand | apply | analyze |
                           evaluate | create
        * blooms_level_int: 1–6  (numeric for range queries)
        * difficulty:      easy | medium | hard
        * difficulty_score: 0.0–1.0

  Batching:
    - CLASSIFICATION_BATCH_SIZE = 10 chunks per API call.
    - MAX_CHUNK_TEXT_CHARS = 600 chars max sent per chunk (for cost control).
    - Falls back to safe defaults on any API error.

  Final store:
    - Classification data is merged back into the chunk records in PostgreSQL.
    - blooms_level_int and difficulty_score are indexed in Qdrant payload for
      fast range filtering during retrieval.

  Note: Step 10 (Align / concept mapping) originally used Gemini but is now
  merged into Step 9's academic classification via GPT-3.5.

────────────────────────────────────────────────────────────────────────────────
INGESTION PIPELINE SUMMARY FLOW
────────────────────────────────────────────────────────────────────────────────

  [Upload File]
       │
       ▼
  Step 1: PARSE         → raw SemanticElements
       │
       ▼
  Step 2: NORMALIZE     → unicode-clean elements
       │
       ▼
  Step 3: CAPTION       → image text descriptions (GPT-4o)
       │
       ▼
  Step 4: FORMAT TABLES → clean Markdown tables (LLM)
       │
       ▼
  Step 5: CLEANUP       → noise removed (deterministic)
       │
       ▼
  Step 6: CLASSIFY      → category assigned per element
       │
       ▼
  Step 7: CHUNK         → retrieval-optimized DocumentChunkInfo objects
       │
       ▼
  Step 8: EMBED         → 1536-dim vectors (text-embedding-3-small)
       │
       ▼
  Step 9: INDEX         → stored in PostgreSQL + Qdrant
       │
       ▼
  Step 10: ACADEMIC     → Bloom + difficulty + section_type labels (GPT-3.5)
           CLASSIFY        stored back to DB and Qdrant payload


================================================================================
PART 2 — GENERATION PIPELINE
================================================================================

Directory: ai-backend/generation/
Entry:     routers/generation.py  (FastAPI router that orchestrates all steps)

The generation pipeline converts an exam pattern + subject into a full exam
paper with questions, options, answers, and marking schemes.

────────────────────────────────────────────────────────────────────────────────
STEP 1: PATTERN INTERPRETATION  (generation/pattern_interpreter.py)
────────────────────────────────────────────────────────────────────────────────

  Input:  Exam pattern as plain text OR as an uploaded PDF.
  Output: ParsedPattern (structured JSON)

  What it does:
    - If PDF: extracts plain text using pypdf (extract_pdf_text).
    - Sends the pattern text to OpenAI GPT with a specific parsing prompt.
    - GPT returns a JSON object describing each question:
        * question_no    — sequential number
        * units          — which subject units are tested (e.g. [1, 2])
        * marks          — marks for this question
        * question_type  — "mcq" or "descriptive"
        * nature         — basic concepts | application-based | case study |
                           comparison | advanced topics | emerging trends
        * expected_bloom — list of Bloom's levels inferred from the pattern
        * is_or_pair     — true if this is an "or" alternative question
        * or_pair_with   — question_no of the paired alternative

  MCQ detection rules (in normalisation):
    "mcq", "multiple", "objective", "1 mark", "one mark" → question_type = "mcq"
    Everything else → "descriptive"

  Functions:
    - interpret_pattern(pattern_text, total_marks)   → ParsedPattern
    - interpret_pattern_from_pdf(pdf_bytes, total_marks)  → ParsedPattern

────────────────────────────────────────────────────────────────────────────────
STEP 2: BLUEPRINT BUILDING  (generation/blueprint_builder.py)
────────────────────────────────────────────────────────────────────────────────

  Input:  ParsedPattern + subject_id + optional difficulty_pref + DB session
  Output: BlueprintSpec (with QuestionSpec objects ready for retrieval)

  What it does:
    - Maps pattern unit numbers (1, 2, 3 — relative) to real database unit IDs
      (e.g., 41, 42, 43) by querying the DB and ordering units by their `order`
      column.
    - Resolves Bloom's taxonomy targets for each question:
        Priority: explicit expected_bloom from LLM > nature mapping
        Nature → Bloom mappings (deterministic, no LLM):
          "basic concepts"   → [remember, understand]
          "application-based"→ [apply]
          "case study"       → [analyze]
          "comparison"       → [analyze, evaluate]
          "advanced topics"  → [evaluate, create]
          "emerging trends"  → [create]
    - Applies global difficulty preference if not set per-question.
    - Preserves OR-pair linkage between question specs.

  Output fields per QuestionSpec:
    question_no, units (real DB IDs), marks, bloom_targets, difficulty,
    nature, question_type, is_or_pair, or_pair_with

────────────────────────────────────────────────────────────────────────────────
STEP 3: RETRIEVAL  (generation/retrieval_engine.py → retrieve_chunks_for_spec)
────────────────────────────────────────────────────────────────────────────────

  Input:  DB session + QuestionSpec + subject_id + exclude_chunk_ids
  Output: List[DocumentChunk]  (top diverse, bloom-aligned chunks)

  What it does:
    - HYBRID SEARCH: Combines BM25 (keyword) + vector (semantic) search.
    - FILTERING:
        * subject_id must match
        * unit_id must be IN spec.units
        * blooms_level_int must fall within the Bloom range for the question
        * difficulty must match spec.difficulty
    - USAGE PENALTY: Chunks that have been used many times get a score penalty:
        score = score / (1 + usage_count * USAGE_PENALTY_FACTOR)
    - EXCLUSION: Chunks already used in this generation run are excluded
      (via exclude_chunk_ids list) to prevent the same content repeating
      across different questions.
    - MMR (Maximal Marginal Relevance): After initial retrieval, selects the
      final top-k chunks to maximize both relevance and diversity.
        * lambda_ = 0.7 (balance between relevance and diversity)
        * Cosine similarity used to measure redundancy between candidates.

  Constants:
    FINAL_CHUNK_COUNT = 5   (chunks returned per question)
    MMR_LAMBDA = 0.7
    USAGE_PENALTY_FACTOR = 0.1

────────────────────────────────────────────────────────────────────────────────
STEP 4: QUESTION GENERATION  (generation/question_generator.py)
────────────────────────────────────────────────────────────────────────────────

  Input:  QuestionSpec + List[DocumentChunk]
  Output: GeneratedQuestion

  What it does:
    - Routes to MCQ or descriptive generation based on spec.question_type.
    - Formats retrieved chunks into a numbered context block sent to the LLM.
    - Calls OpenAI GPT with a structured prompt specifying:
        * Bloom's level target
        * Marks count
        * Difficulty
        * Question nature/topic
        * Target units
        * Context text (retrieved chunks, labelled [1], [2], ...)
    - LLM is instructed to respond with ONLY a valid JSON object.

  MCQ generation prompt output format:
    {
      "question_text": "...",
      "bloom_level": "...",
      "difficulty": "...",
      "marks": N,
      "options": [
        {"label": "A", "text": "..."},
        {"label": "B", "text": "..."},
        {"label": "C", "text": "..."},
        {"label": "D", "text": "..."}
      ],
      "correct_answer": "A",
      "explanation": "..."
    }

  Descriptive generation prompt output format:
    {
      "question_text": "...",
      "bloom_level": "...",
      "difficulty": "...",
      "marks": N,
      "answer_key": "...",  (model answer / key points)
      "marking_scheme": [
        {"point": "...", "marks": N},
        ...
      ]
    }

  Fallback:
    - If generation completely fails, _fallback_question() returns a safe
      placeholder question instead of crashing the pipeline.

────────────────────────────────────────────────────────────────────────────────
STEP 5: CO MAPPING  (generation/co_mapper.py)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Maps each unit_id to a Course Outcome (CO) label (e.g., "CO1", "CO2").
    - CO labels are stored in the database (units table or a co_map table).
    - Returns a dict: {unit_id: co_label}

────────────────────────────────────────────────────────────────────────────────
STEP 6: PAPER ASSEMBLY  (generation/paper_assembler.py → assemble_paper)
────────────────────────────────────────────────────────────────────────────────

  Input:  BlueprintSpec + questions_per_spec dict + co_map dict
  Output: PaperOutput

  What it does:
    - Iterates through all QuestionSpec objects in the blueprint.
    - Resolves CO label for each question from co_map using the primary unit.
    - For OR-pair questions (e.g., "Q1 OR Q2"):
        * Groups both questions as variants under a single PaperSection.
        * Labels them "Q1" and "Q2" for display.
    - For normal (non-OR) questions:
        * Creates a single PaperSection with one GeneratedVariant.
    - Sorts all sections by question_no.
    - Computes generation metadata:
        * question_count
        * or_pairs count
        * bloom_distribution per level

  Output schema (PaperOutput):
    - subject_id
    - total_marks
    - sections: List[PaperSection]
        Each section has:
          question_no, marks, co_mapped, bloom_level,
          variants: List[GeneratedVariant]
            Each variant has: variant_label, question (GeneratedQuestion)
    - created_at
    - generation_metadata

────────────────────────────────────────────────────────────────────────────────
STEP 7: VALIDATION  (generation/validator.py)
────────────────────────────────────────────────────────────────────────────────

  What it does:
    - Validates the assembled PaperOutput for:
        * Correct total marks (sum of all section marks)
        * All questions present (no missing question numbers)
        * OR-pair consistency (both variants present)
        * Bloom's taxonomy level validity
    - Returns ValidationResult with pass/fail and any warnings.

────────────────────────────────────────────────────────────────────────────────
GENERATION PIPELINE SUMMARY FLOW
────────────────────────────────────────────────────────────────────────────────

  [Exam Pattern (text or PDF)] + [Subject ID] + [Difficulty]
       │
       ▼
  Step 1: PATTERN         → ParsedPattern
          INTERPRET           (GPT-4o / GPT-4)
       │
       ▼
  Step 2: BLUEPRINT       → BlueprintSpec  (QuestionSpec per question)
          BUILD               (deterministic, no LLM)
       │
       ▼
  For each QuestionSpec:
       │
       ├── Step 3: RETRIEVE → List[DocumentChunk]
       │           CHUNKS      (hybrid BM25+vector, MMR diversity,
       │                        usage penalty, exclusion list)
       │
       └── Step 4: GENERATE → GeneratedQuestion
                   QUESTION    (GPT-4o, MCQ or descriptive)
       │
       ▼
  Step 5: CO MAP          → Dict[unit_id → CO label]
       │
       ▼
  Step 6: ASSEMBLE        → PaperOutput
          PAPER               (groups OR-pairs, sorts sections)
       │
       ▼
  Step 7: VALIDATE        → ValidationResult
       │
       ▼
  [Return completed exam paper to frontend API]


================================================================================
KEY DESIGN PRINCIPLES
================================================================================

  1. SEPARATION OF CONCERNS
     Each step of both pipelines is implemented in its own module and can run
     independently. This makes debugging, testing, and replacing individual
     steps straightforward.

  2. LLM ISOLATION
     LLM calls are contained to specific steps only:
       - Ingestion: Step 3 (image captions, GPT-4o), Step 4 (table format, LLM),
                    Step 10 (academic classification, GPT-3.5)
       - Generation: Step 1 (pattern interpretation, GPT), Step 4 (question
                     generation, GPT-4o)
     All other steps are deterministic.

  3. HYBRID RETRIEVAL
     The retrieval engine combines BM25 (keyword matching) with vector
     (semantic) search, then applies:
       - Bloom-level filtering to ensure difficulty alignment
       - Usage penalties to avoid repeatedly using the same chunks
       - Cross-question exclusion to ensure distinct content per question
       - MMR to ensure diversity within the chunks chosen for one question

  4. ACADEMIC METADATA
     Every chunk is enriched with:
       - Bloom's level (remember→create)
       - Difficulty (easy/medium/hard)
       - Section type (definition/example/derivation/etc.)
       - Source type (textbook/slides/notes/etc.)
       - Section path (unit > topic > subtopic hierarchy)
     This metadata enables precise, filtered retrieval at generation time.

  5. FAULT TOLERANCE
     Both pipelines have fallback mechanisms:
       - Ingestion: academic_classifier falls back to safe defaults on API error
       - Generation: question_generator returns placeholder questions on failure
       - Paper assembler inserts placeholder sections for missing questions


================================================================================
FILES QUICK REFERENCE
================================================================================

  INGESTION:
    ingestion/__init__.py          — Pipeline package, exports all steps
    ingestion/parser.py            — Step 1: Document parsing (unstructured)
    ingestion/normalizer.py        — Step 2: Unicode/whitespace cleanup
    ingestion/image_captioner.py   — Step 3: GPT-4o image captions
    ingestion/table_formatter.py   — Step 4: LLM Markdown table formatting
    ingestion/cleanup.py           — Step 5: Deterministic noise removal
    ingestion/classifier.py        — Step 6: Element category classification
    ingestion/chunker.py           — Step 7: Section-aware chunking
    embeddings/generator.py        — Step 8: text-embedding-3-small vectors
    embeddings/qdrant_manager.py   — Step 9: Qdrant + PostgreSQL indexing
    ingestion/academic_classifier.py — Step 10: GPT-3.5 Bloom/difficulty labelling
    ingestion/schemas.py           — SemanticElement and related data schemas

  GENERATION:
    generation/pattern_interpreter.py — Step 1: Exam pattern → ParsedPattern
    generation/blueprint_builder.py   — Step 2: ParsedPattern → BlueprintSpec
    generation/retrieval_engine.py    — Step 3: Hybrid chunk retrieval (MMR)
    generation/question_generator.py  — Step 4: GPT question generation
    generation/co_mapper.py           — Step 5: Unit → CO label mapping
    generation/paper_assembler.py     — Step 6: Assemble PaperOutput
    generation/validator.py           — Step 7: Paper validation
    generation/gpt_client.py          — Shared OpenAI GPT client wrapper
    generation/usage_tracker.py       — Track chunk usage counts
    generation/schemas.py             — All generation data schemas

  ROUTING:
    routers/generation.py            — FastAPI endpoint that drives the
                                       generation pipeline end-to-end
    routers/ingestion.py (or similar)— FastAPI endpoint that drives ingestion

================================================================================
